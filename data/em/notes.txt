# Note
the python env is in wsl. open command prompt and type
wsl
cd /mnt/c/Users/z5440219/OneDrive - UNSW/Desktop/github/cloned/RollingDepth or cd ~/projects/RollingDepth 
conda activate rd9

# Reduce fps
ffmpeg -i .\o.mp4 -r 10 o_10fps.mp4

# Split video in chunks
python video_splitter.py records_10fps/o_10fps.mp4 0:05 0:32 0:57 1:28 1:50 2:19 2:40 3:04 3:30 3:46 4:06 4:30 -o chunks -p record 

# Run prediction
python run_video.py -i data/em/chunks -o data/em/output -p full --res 0 --restore-res True --verbose --frames 550 --max-vae-bs 1 --unload-snippet true

--frames XXX is the main limiting factor, try to take it a small a possible but bigger than the duration of the longer chunk

# Prepare slam input folder
python create_orb_slam_input.py --input-video-dir FOLDER_WITH_VIDEOS --depth-dir output --output-slam-dir slam --depth-scale 5000.0

# Utils
source ~/miniconda3/bin/activate


# Depth tranformations
BronchoSim saves depth as:
# raw depth comes normalized from 0 to 1
depth = get_depth_image(self.depthTex) in [0,1]
# linearize it with near and far planes (0.1, 1000.0)
depth_linear = (2.0 * near_plane * far_plane) / (far_plane + near_plane - (2.0 * depth - 1.0) * (far_plane - near_plane))
# multiply by depth factor for ORBSLAM use 
depth_slam = (depth_linear * depth_map_factor).astype(np.uint16)

RollingDepth predicts depth as:
inverse_depth = 1/depth
pred = RD(rgb) in [-1,1]
pred * scale + shift = 1/gt_depth

So to compare them I have to
# divide the gt by depth_map_factor
gt_depth_meter = gt_depth / depth_map_factor
# compute inverse depth prediction
inv_pred = pred * scale + shift
depth_meter = 1 / inv_pred
# compare 
MAE/RMSE(gt_depth_meter,depth_meter)



